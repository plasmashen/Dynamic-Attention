{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52231d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, tqdm, math, re, string,random\n",
    "import numpy as np\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from torch.nn import CrossEntropyLoss, CosineSimilarity\n",
    "from torch.utils.data import DataLoader, SequentialSampler, TensorDataset, Dataset\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa6085a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:1')\n",
    "\n",
    "def sent_pred(sent):\n",
    "    encoded_dict = tokenizer(sent,add_special_tokens = True,max_length = 256,\n",
    "                                         return_attention_mask = True,return_tensors = 'pt', truncation=True)   \n",
    "    iids = encoded_dict['input_ids'].to(device)\n",
    "    amasks = encoded_dict['attention_mask'].to(device)\n",
    "    p = bert(iids, token_type_ids=None, attention_mask=amasks)[0]\n",
    "    return torch.argmax(p).item()\n",
    "\n",
    "def sentences_pred(sents, batch_size=32):\n",
    "    encoded_dict = tokenizer(sents,add_special_tokens = True, max_length = 256, padding='max_length', \n",
    "                             return_attention_mask = True, return_tensors = 'pt',truncation=True)   \n",
    "    bert.to(device)\n",
    "    bert.eval()\n",
    "    input_ids = encoded_dict['input_ids'].to(device)\n",
    "    input_mask = encoded_dict['attention_mask'].to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = bert(input_ids, token_type_ids=None, attention_mask=input_mask)[0]\n",
    "        probs = nn.functional.softmax(logits, dim=-1)\n",
    "    return probs\n",
    "    return torch.cat(probs_all, dim=0)\n",
    "\n",
    "\n",
    "def importscore(text, sample=0):\n",
    "    random.seed(2020)\n",
    "    text_ls = text.split()\n",
    "    text_ls = text_ls[:200]\n",
    "#     no_text_ls = [(i,j) for i,j in enumerate(text_ls)]\n",
    "    len_text = len(text_ls)\n",
    "    if 0<sample<len_text:\n",
    "        leave_1_texts = [' '.join(text_ls[:ii] + text_ls[min(ii + 1, len_text):]) \n",
    "                         for ii in random.sample(list(range(len_text)),sample)]\n",
    "    else:\n",
    "        leave_1_texts = [' '.join(text_ls[:ii] + text_ls[min(ii + 1, len_text):]) for ii in range(len_text)]\n",
    "    leave_1_probs = sentences_pred([text]+leave_1_texts)\n",
    "    orig_probs = leave_1_probs[:1].squeeze()\n",
    "    orig_label = torch.argmax(orig_probs)\n",
    "    orig_prob = orig_probs.max()\n",
    "    leave_1_probs_argmax = torch.argmax(leave_1_probs[1:], dim=-1)\n",
    "    import_scores = (orig_prob - leave_1_probs[1:, orig_label]).data.cpu().numpy()\n",
    "    return import_scores\n",
    "\n",
    "def cal_AUC(orig_sent, adv_sent, k=0):\n",
    "    nms, nms1 = [], []\n",
    "    for i in tqdm.tqdm(range(int(len(orig_sent)))):\n",
    "        isa = importscore(adv_sent[i],k)\n",
    "        iso = importscore(orig_sent[i],k)\n",
    "        nms.append({'idx': i, 'isa': isa, 'iso': iso})\n",
    "    for i in nms:\n",
    "        isa_ = sorted(i['isa'],reverse=True)\n",
    "        iso_ = sorted(i['iso'],reverse=True)\n",
    "        entropyo = -np.mean([abs(i)*np.log(abs(i)) for i in iso_[:] if i!=0])\n",
    "        entropya = -np.mean([abs(i)*np.log(abs(i)) for i in isa_[:] if i!=0])\n",
    "    #     stdo, stda = np.std(iso_[:128]), np.std(isa_[:128])\n",
    "        nms1.append({'idx': i['idx'], 'entropyo': entropyo, 'entropya':entropya})\n",
    "    preds = [i['entropyo']for i in nms1]+[i['entropya']for i in nms1]\n",
    "    y_test = [0]*len(nms1)+[1]*len(nms1)\n",
    "    y_test = label_binarize(y_test, classes=[0, 1])\n",
    "    fpr, tpr, threshold = metrics.roc_curve(y_test,preds)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    return roc_auc\n",
    "    \n",
    "def clean_str(string, TREC=False):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Every dataset is lower cased except for TREC\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip() if TREC else string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "81eefefb",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "add_code_sample_docstrings() got an unexpected keyword argument 'tokenizer_class'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_33089/3635953457.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"..\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m from model.sequence_classification import (\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mBertPrefixForSequenceClassification\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mBertPromptForSequenceClassification\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/P-tuning-v2/model/sequence_classification.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprefix_encoder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPrefixEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeberta\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDebertaModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDebertaPreTrainedModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mContextPooler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStableDropout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/P-tuning-v2/model/deberta.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    892\u001b[0m     \u001b[0mDEBERTA_START_DOCSTRING\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m )\n\u001b[0;32m--> 894\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mDebertaModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDebertaPreTrainedModel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    895\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/P-tuning-v2/model/deberta.py\u001b[0m in \u001b[0;36mDebertaModel\u001b[0;34m()\u001b[0m\n\u001b[1;32m    920\u001b[0m         \u001b[0mcheckpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_CHECKPOINT_FOR_DOC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    921\u001b[0m         \u001b[0moutput_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSequenceClassifierOutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 922\u001b[0;31m         \u001b[0mconfig_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_CONFIG_FOR_DOC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    923\u001b[0m     )\n\u001b[1;32m    924\u001b[0m     def forward(\n",
      "\u001b[0;31mTypeError\u001b[0m: add_code_sample_docstrings() got an unexpected keyword argument 'tokenizer_class'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from model.sequence_classification import (\n",
    "    BertPrefixForSequenceClassification,\n",
    "    BertPromptForSequenceClassification,\n",
    "    RobertaPrefixForSequenceClassification,\n",
    "    RobertaPromptForSequenceClassification,\n",
    "    DebertaPrefixForSequenceClassification\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8f7bc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert = AutoModelForSequenceClassification.from_pretrained('../checkpoints/amazon-bert-normal')\n",
    "# tokenizer = AutoTokenizer.from_pretrained('../checkpoints/amazon-bert-normal', do_lower_case=True);\n",
    "# bert = BertPrefixForSequenceClassification.from_pretrained('../checkpoints/amazon-bert-prefix')\n",
    "# tokenizer = AutoTokenizer.from_pretrained('../checkpoints/amazon-bert-prefix', do_lower_case=True);\n",
    "bert = BertPromptForSequenceClassification.from_pretrained('../checkpoints/amazon-bert-prompt')\n",
    "tokenizer = AutoTokenizer.from_pretrained('../checkpoints/amazon-bert-prompt', do_lower_case=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8eb11e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of successful attacks: 250\n",
      "Number of failed attacks: 215\n",
      "Number of skipped attacks: 35\n",
      "Original accuracy: 93.0%\n",
      "Accuracy under attack: 43.0%\n",
      "Attack success rate: 53.76%\n",
      "Average perturbed word %: 3.74%\n",
      "Average num. words per input: 82.49\n",
      "Avg num queries: 633.99\n"
     ]
    }
   ],
   "source": [
    "f = open('adv_output/amazon-bert-normal/pwws/2022-07-01-23-34-log.txt')\n",
    "txt = f.read()\n",
    "text = txt.split('--------------------------------------------- Result ')\n",
    "for i in text[-1].split('\\n')[-10:-1]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "34babcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_sent_, adv_sent_, nms, nms2 = [], [], [], []\n",
    "count = 0\n",
    "for i in range(1,501):\n",
    "    tmp0 = text[i].split('\\n')\n",
    "    if 'FAILED' in tmp0[1] or 'SKIPPED' in tmp0[1]:\n",
    "        pass\n",
    "    else:\n",
    "        orig_sent_.append(tmp0[3])\n",
    "        adv_sent_.append(tmp0[5])\n",
    "orig_sent = [i.replace('[','').replace(']','') for i in orig_sent_]\n",
    "adv_sent = [i.replace('[','').replace(']','') for i in adv_sent_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f7ae070",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 136/136 [00:48<00:00,  2.81it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 136/136 [00:00<00:00, 4907.56it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm.tqdm(range(int(len(orig_sent)))):\n",
    "    isa = importscore(adv_sent[i],20)\n",
    "    iso = importscore(orig_sent[i],20)\n",
    "    nms.append({'idx': i, 'isa': isa, 'iso': iso})\n",
    "for i in tqdm.tqdm(nms):\n",
    "    isa_ = sorted(i['isa'],reverse=True)\n",
    "    iso_ = sorted(i['iso'],reverse=True)\n",
    "    entropyo = -np.mean([abs(i)*np.log(abs(i)) for i in iso_[:] if i!=0])\n",
    "    entropya = -np.mean([abs(i)*np.log(abs(i)) for i in isa_[:] if i!=0])\n",
    "#     stdo, stda = np.std(iso_[:128]), np.std(isa_[:128])\n",
    "    nms2.append({'idx': i['idx'], 'entropyo': entropyo, 'entropya':entropya})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ced73b3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8256920415224913"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = [i['entropyo']for i in nms2]+[i['entropya']for i in nms2]\n",
    "y_test = [0]*len(nms2)+[1]*len(nms2)\n",
    "y_test = label_binarize(y_test, classes=[0, 1])\n",
    "fpr, tpr, threshold = metrics.roc_curve(y_test,preds)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6c8d7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt2",
   "language": "python",
   "name": "pt2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
